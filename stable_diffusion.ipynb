{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ccdba96d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Programs\\Python\\3.12.7\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from diffusers import StableDiffusionPipeline, UNet2DConditionModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1fe4672b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 43\u001b[39m\n\u001b[32m     40\u001b[39m loader = DataLoader(dataset, batch_size=\u001b[32m2\u001b[39m, shuffle=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     42\u001b[39m \u001b[38;5;66;03m# solo 500 imagenes\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m loader = DataLoader(\u001b[43mtorch\u001b[49m.utils.data.Subset(dataset, \u001b[38;5;28mrange\u001b[39m(\u001b[32m500\u001b[39m)), batch_size=\u001b[32m4\u001b[39m, shuffle=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     45\u001b[39m \u001b[38;5;66;03m# Mostrar ejemplos\u001b[39;00m\n\u001b[32m     46\u001b[39m batch = \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(loader))\n",
      "\u001b[31mNameError\u001b[39m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "class SketchToAnimeDataset(Dataset):\n",
    "    def __init__(self, sketch_dir, anime_dir, image_size=512):\n",
    "        self.sketch_dir = sketch_dir\n",
    "        self.anime_dir = anime_dir\n",
    "        self.sketches = sorted(os.listdir(sketch_dir))\n",
    "        self.animes = sorted(os.listdir(anime_dir))\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((image_size, image_size)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5], [0.5])\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sketches)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sketch_path = os.path.join(self.sketch_dir, self.sketches[idx])\n",
    "        anime_path = os.path.join(self.anime_dir, self.animes[idx])\n",
    "\n",
    "        sketch = Image.open(sketch_path).convert(\"RGB\")\n",
    "        anime = Image.open(anime_path).convert(\"RGB\")\n",
    "\n",
    "        return {\n",
    "            \"sketch\": self.transform(sketch),\n",
    "            \"anime\": self.transform(anime)\n",
    "        }\n",
    "\n",
    "# Inicializamos dataset\n",
    "# dataset = SketchToAnimeDataset(\"dataset/sketches\", \"dataset/anime\")\n",
    "# loader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "\n",
    "# Ejemplo de uso\n",
    "dataset = SketchToAnimeDataset(r\"D:\\Ciencias\\Drawnime\\data\\train\\sketches\", r\"D:\\Ciencias\\Drawnime\\data\\train\\faces\")\n",
    "loader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "# solo 500 imagenes\n",
    "loader = DataLoader(torch.utils.data.Subset(dataset, range(500)), batch_size=4, shuffle=True)\n",
    "\n",
    "# Mostrar ejemplos\n",
    "batch = next(iter(loader))\n",
    "plt.figure(figsize=(10, 4))\n",
    "for i in range(4):\n",
    "    plt.subplot(2, 4, i+1)\n",
    "    plt.imshow(((batch[\"sketch\"][i].permute(1, 2, 0) * 0.5) + 0.5))\n",
    "    plt.title(\"Sketch\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.subplot(2, 4, i+5)\n",
    "    plt.imshow(((batch[\"anime\"][i].permute(1, 2, 0) * 0.5) + 0.5))\n",
    "    plt.title(\"Anime\")\n",
    "    plt.axis(\"off\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0106a7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Loading pipeline components...: 100%|██████████| 7/7 [00:00<00:00, 16.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parámetros LoRA entrenables: 859,520,964\n"
     ]
    }
   ],
   "source": [
    "from diffusers import StableDiffusionPipeline\n",
    "from diffusers.models.attention_processor import LoRAAttnProcessor\n",
    "import torch\n",
    "import os\n",
    "\n",
    "model_id = \"runwayml/stable-diffusion-v1-5\"\n",
    "\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.float16,\n",
    "    variant=\"fp16\"\n",
    ").to(\"cuda\")\n",
    "\n",
    "# Reducir uso de VRAM\n",
    "pipe.enable_attention_slicing()\n",
    "pipe.enable_vae_tiling()\n",
    "pipe.unet.enable_gradient_checkpointing()\n",
    "\n",
    "# Crear y asignar procesadores LoRA a todas las capas de atención\n",
    "lora_attn_procs = {}\n",
    "for name in pipe.unet.attn_processors.keys():\n",
    "    lora_attn_procs[name] = LoRAAttnProcessor()\n",
    "\n",
    "pipe.unet.set_attn_processor(lora_attn_procs)\n",
    "\n",
    "# Verificamos parámetros entrenables\n",
    "trainable_params = sum(p.numel() for p in pipe.unet.parameters() if p.requires_grad)\n",
    "print(f\"Parámetros LoRA entrenables: {trainable_params:,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6bc3833",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Animetx\\AppData\\Local\\Temp\\ipykernel_16624\\667150188.py:28: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()  # para AMP seguro\n",
      "Epoch 1/5:   0%|          | 1/500 [00:02<22:24,  2.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 1/5 — Loss: 1.2891\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:   0%|          | 2/500 [00:12<54:51,  6.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 1/5 — Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:   1%|          | 3/500 [00:20<1:02:44,  7.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 1/5 — Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:   1%|          | 4/500 [00:28<1:04:22,  7.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 1/5 — Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:   1%|          | 4/500 [00:36<1:16:15,  9.22s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 57\u001b[39m\n\u001b[32m     54\u001b[39m         loss.backward()\n\u001b[32m     55\u001b[39m         optimizer.step()\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m✅ Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m — Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     59\u001b[39m \u001b[38;5;66;03m# Guardar LoRA\u001b[39;00m\n\u001b[32m     60\u001b[39m os.makedirs(\u001b[33m\"\u001b[39m\u001b[33mlora_anime\u001b[39m\u001b[33m\"\u001b[39m, exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from torch import nn, optim\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "optimizer = optim.AdamW(\n",
    "    [p for p in pipe.unet.parameters() if p.requires_grad],\n",
    "    lr=1e-4\n",
    ")\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "epochs = 5\n",
    "pipe.unet.train()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for batch in tqdm(loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "        sketch = batch[\"sketch\"].to(\"cuda\", dtype=torch.float16)\n",
    "        anime = batch[\"anime\"].to(\"cuda\", dtype=torch.float16)\n",
    "\n",
    "        with torch.cuda.amp.autocast():\n",
    "            # Codificar el sketch con el VAE\n",
    "            latents = pipe.vae.encode(sketch).latent_dist.sample() * 0.18215\n",
    "\n",
    "            # Predicción UNet (ruido → imagen anime)\n",
    "            noise_pred = pipe.unet(latents, timestep=torch.tensor([0], device=\"cuda\")).sample\n",
    "\n",
    "            # Target: latentes del anime\n",
    "            target_latents = pipe.vae.encode(anime).latent_dist.sample() * 0.18215\n",
    "\n",
    "            loss = loss_fn(noise_pred, target_latents)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs} - Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Guardamos LoRA entrenado\n",
    "os.makedirs(\"lora_anime\", exist_ok=True)\n",
    "pipe.unet.save_attn_procs(\"lora_anime\")\n",
    "print(\"✅ LoRA guardado en ./lora_anime/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247b8bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import StableDiffusionImg2ImgPipeline\n",
    "from PIL import Image\n",
    "\n",
    "pipe = StableDiffusionImg2ImgPipeline.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.float16\n",
    ").to(\"cuda\")\n",
    "\n",
    "pipe.unet.load_attn_procs(\"./lora-anime-enhancer\")\n",
    "\n",
    "image = Image.open(\"test_sketch.png\").convert(\"RGB\")\n",
    "prompt = \"high quality anime style, clean lines, vibrant colors\"\n",
    "\n",
    "result = pipe(prompt=prompt, image=image, strength=0.7, guidance_scale=7.5)\n",
    "result.images[0].save(\"output.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
